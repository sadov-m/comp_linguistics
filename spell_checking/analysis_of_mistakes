Пономарёва Маша, Садов Миша

Опробованные нами подходы можно условно разделить на 3 группы: 
-	подходы на основе сторонней библиотеки для Python под названием python-levenshtein;
-	подходы, основанные на написанном нами алгоритме Левенштейна (итеративный, с двумя рядами матрицы);
-	подходы, основанные на попытке применить Morfessor (инструмент для автоматического разбиения слова на части речи), проверяя части слов вместо целых слов;

Данные (для словаря, из которого в дальнейшем подбирались кандидаты на исправление опечатки):
-	НКРЯ: униграммы и биграммы с сайта и их частотности (http://www.ruscorpora.ru/corpora-freq.html)
-	НКРЯ: словарь, собранный из всего корпуса
Все словоформы были приведены к нижнему регистру. С одной стороны, это решение создаёт проблемы, так как включает имена собственные и аббревиатуры в список потенциальных исправлений для обычных слов (пример из реальных данных, когда слово “ширена” было исправлено на “Ирена”), однако было установлено, что в исходных данных некоторые общеупотребительные слова были написаны с большой буквы, потому было решено не хранить в словах различия по регистру. Кроме того, при принятии данного решения были учтены особенности набора данных для проверки (а он был приведён к нижнему регистру и не содержал знаков препинания).

Python-levenshtein(levenshtein)
Было использовано две стандартных метрики для измерения дистанции между строками: расстояние Левенштейна и расстояние Джаро-Уинклера (нечёткий поиск, https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance). Мотивацией использовать эту метрику стал тот факт, что расстояние Джаро, на котором и основана метрика Джаро-Уинклера, использует формулу, основанную на количестве общих символов (то есть при, к примеру, изменении порядка символов местами - а это очень частая опечатка - метрика близости всё равно будет достаточно высокой), а дополнительный вес, введённый в формулу, даёт больший балл тем строкам, которые имеют достаточно длинные совпадающие последовательности в начале слов, что тоже вполне логично, ведь обычно начало слов пользователи пишут правильно.
В самом начале исправления опечаток на вход подавалась строка, токенизатор был простой: разбиение строки по пробелам с помощью регулярного выражения, так как данные не содержали сложных случаев токенизации, и можно было с уверенностью утверждать, что даже такое простое решение сработает максимально качественно.
Затем определялось, необходимо ли проверять слово на предмет наличия опечаток. Не проверялись токены длиной 3 и меньше (вероятность, что опечатка будет совершена в таком коротком слове мала, а вероятность заменить аббревиатуру), токены, имеющиеся в словаре, и токены, состоящие полностью из цифр.
Если же слово попадало в категорию проверяемых, в дальнейшем проводилась первичная фильтрация кандидатов на его исправление с помощью измерения расстояния Левенштейна.
Так как оригинальный метод, взятый из библиотеки, не давал весов различным операциям и никаким образом не учитывал длину исправляемого слово или кандидата, первоначальным решением было нормировать метрику на длину исправляемого слова, однако вскоре появилась проблема того, что более длинные слова получали преимущество, даже если не являлись оптимальными кандидатами. В связи с этим было решено поступить иначе: нормировать расстояние Левенштейна на сумму длины исправляемого слова и десятичного логарифма от суммы длины токена и модуля от разности длины токена и длины кандидата на его слово-исправление.
Затем список кандидатов на исправление опечатки был отфильтрован по полученной метрике: нормированное расстояние должно было быть не более 2, делённых на нормирующий коэффициент.
Сначала метрику Джаро-Уинклера предполагалось использовать как альтернативу Левенштейну, однако ощутимого прироста качества за счёт её использования добиться не удалось. Была предпринята попытка использовать её для отсеивания кандидатов для списка, полученного расчётом нормированного расстояния Левенштейна, однако ощутимого прироста качества достичь снова не удалось. Также были предприняты попытки выбрать оптимального кандидата по биграммам из НКРЯ и их частотам, однако это даже испортило результаты ввиду того, что зачастую приходилось сравнивать биграммы с опечатками, что давало нули. В итоге было решено брать кандидатов с наименьшим расстоянием по нормированному расстоянию Левенштейна.

Достоинства: быстрота работы (загрузка словаря примерно 2.5 секунды, примерно секунда на исправление одного слова с опечаткой), нормализация оригинальной формулы.
Недостатки: не было применено эвристик и морфологического анализа для снижения перплексии и получения однозначно корректного кандидата, “грязный” словарь, содержащий опечатки, неудовлетворительная работа с “немеханическими” опечатками.
Возможные улучшения: расширение словаря и его очистка от опечаток, имплементация биграмм частей речи, эвристик и морфологического анализа для согласования.

Самостоятельная имплементация Левенштейна (hand_written_levenshtein)
Достаточно классическая реализация алгоритма с итеративным проходом по двум строкам матрицы. Были предприняты попытки ввести наиболее частотные опечатки для каждого символа в виде символа ниже, выше, слева и справа на одну позицию (0.5 - вес для таких опечаток, 1 - для прочих). Однако “грязный” словарь, содержащий опечатки, помешал добиться улучшения качества при применении этого подхода.
Данный подход не был развит далее, так как показал неудовлетворительное время работы: около 1,5 минут приходилось на исправление одного слова.

Подходы, основанные на применении Morfessor
Идеей данного подхода было обучить модель для автоматической сегментации слова на его части и затем применить её к исправлению опечаток. Таким образом, была обучена модель и был получен список сегментов, которые она встретила во время обучения. Далее предполагалось с помощью модели предсказать разбиение слова с опечатками, найти те сегменты, что не содержат ошибок, получить оптимальных кандидатов для исправления сегментов с ошибками, затем путём их перебора составить все возможные комбинации слова и найти в словаре существующие варианты.
Однако реализовать данный подход не вышло. Кроме того, логично предположить, что ввиду “грязности” словаря, данный подход не помог бы найти одного оптимального кандидата на исправление, и пришлось снова бы выбирать его случайно.
